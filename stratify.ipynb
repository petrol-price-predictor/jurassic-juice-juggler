{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import pyarrow\n",
    "import random\n",
    "\n",
    "from pathlib import Path\n",
    "import os\n",
    "from collections import deque\n",
    "from src.config.paths import ROOT_DIR, SAMPLE_DIR, PRICES_DIR, META_DIR\n",
    "\n",
    "import src.fileutils as files\n",
    "import src.visualization as viz\n",
    "import src.process as process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations_info_file = ROOT_DIR / 'data' / 'stations.csv'\n",
    "sample_file_location = SAMPLE_DIR\n",
    "sample_price_location = SAMPLE_DIR / 'prices'\n",
    "\n",
    "RSEED = 42\n",
    "random.seed(RSEED)\n",
    "np.random.seed(RSEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2014\\10\\2014-10-26-prices.csv\n",
    "# 2015\\03\\2015-03-29-prices.csv\n",
    "# 2016\\05\\2016-05-01-prices.csv\n",
    "# 2018\\03\\2018-03-25-prices.csv\n",
    "# 2018\\10\\2018-10-28-prices.csv\n",
    "# 2020\\03\\2020-03-29-prices.csv\n",
    "# 2020\\10\\2020-10-25-prices.csv\n",
    "# 2021\\03\\2021-03-28-prices.csv\n",
    "# 2021\\10\\2021-10-31-prices.csv\n",
    "# 2022\\03\\2022-03-27-prices.csv\n",
    "# 2022\\10\\2022-10-30-prices.csv\n",
    "# 2023\\03\\2023-03-26-prices.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_closing_prices(prices_df):\n",
    "\n",
    "    return prices_df.groupby(level='station').tail(1)\n",
    "\n",
    "\n",
    "def impute_closing_prices(new_prices, closing_prices):\n",
    "\n",
    "    opening_prices = new_prices.groupby(level='station').head(1).reset_index(level=1)\n",
    "    opening_prices = opening_prices.fillna(closing_prices.reset_index(level=1))\n",
    "\n",
    "    # set the datetime index back to where it was and update the new prices with the opening prices\n",
    "    opening_prices = opening_prices.set_index('date', append=True)\n",
    "    new_prices.update(opening_prices, overwrite = False)\n",
    "    return new_prices\n",
    "\n",
    "\n",
    "def fill_missing_prices(prices_df, method='ffill'):\n",
    "    prices_df[['diesel', 'e5', 'e10']] = prices_df \\\n",
    "        .groupby(level='station')[['diesel', 'e5', 'e10']] \\\n",
    "        .fillna(method=method)\n",
    "    \n",
    "    return prices_df\n",
    "\n",
    "####################################\n",
    "\n",
    "# INSTANTIATE EVERYTHING OF THIS\n",
    "prices_meta = pd.DataFrame()\n",
    "closing_prices = pd.DataFrame()\n",
    "last_closing_prices = pd.DataFrame()\n",
    "\n",
    "\n",
    "# Read Data\n",
    "prices_df_raw = pd.read_csv(PRICES_DIR / '2020' / '03' / '2020-03-29-prices.csv')\n",
    "prices_df_raw2 = pd.read_csv(PRICES_DIR / '2020' / '03' / '2020-03-30-prices.csv')\n",
    "dus_stations = pd.read_csv(SAMPLE_DIR / 'stations' / 'stations_dus_plus.csv')\n",
    "\n",
    "# Create a set of all UUIDs in the DUS subsample\n",
    "dus_station_uuid = set(dus_stations.uuid)\n",
    "\n",
    "####################################\n",
    "# PROCESS DAY 1\n",
    "\n",
    "# Drop the 'change' columns for now as they dont provide us with any insight. FUTURE FEATURE ENGINEERING\n",
    "# First Processing Step for Day 1: Drop all but DUS, generate panel\n",
    "prices_df = prices_df_raw.drop(columns=prices_df_raw.filter(like='change').columns)\n",
    "prices_df = prices_df[prices_df.station_uuid.isin(dus_station_uuid)]\n",
    "\n",
    "df = process.extend_panel(prices_df)\n",
    "df = process.swap_sort_index(df)\n",
    "\n",
    "if not last_closing_prices.empty:\n",
    "        df = impute_closing_prices(df, last_closing_prices)\n",
    "df = fill_missing_prices(df)\n",
    "\n",
    "####################################\n",
    "# POSTPROCESS DAY 1: GENERATE METADATA FOR THAT DAY\n",
    "\n",
    "last_closing_prices = get_closing_prices(df)\n",
    "# closing_prices.append_last_closing_prices()\n",
    "\n",
    "####################################\n",
    "# PROCESS DAY 2\n",
    "\n",
    "# First Processing Step for Day 2: Drop all but DUS, generate panel\n",
    "prices_df2 = prices_df_raw2.drop(columns=prices_df_raw2.filter(like='change').columns)\n",
    "prices_df2 = prices_df2[prices_df2.station_uuid.isin(dus_station_uuid)]\n",
    "\n",
    "df2 = process.extend_panel(prices_df2)\n",
    "df2 = process.swap_sort_index(df2)\n",
    "\n",
    "if not last_closing_prices.empty:\n",
    "        df2 = impute_closing_prices(df2, last_closing_prices)\n",
    "df2 = fill_missing_prices(df2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features:\n",
    "# relative prices\n",
    "# opening hours + dummies\n",
    "# holiday dummies\n",
    "# school-holyday dummies\n",
    "# crude oil\n",
    "# with bins: change count/hour\n",
    "\n",
    "# Meta\n",
    "# Average Price per day (per product)\n",
    "# Trade Frequency\n",
    "# was this a holiday\n",
    "# was this a schoolholiday\n",
    "# year\n",
    "# month\n",
    "# day\n",
    "# weekday\n",
    "# average crude oil price that day\n",
    "\n",
    "# Processing\n",
    "# bin dates\n",
    "# Split into 3 prices (more data but faster processing maybe?)\n",
    "# make additional features independent  at first\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>diesel</th>\n",
       "      <th>e5</th>\n",
       "      <th>e10</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>station</th>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">005056ba-7cb6-1ed2-bceb-7e82e4910d2a</th>\n",
       "      <th>2020-03-30 00:02:10+02:00</th>\n",
       "      <td>1.079</td>\n",
       "      <td>1.209</td>\n",
       "      <td>1.179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-30 00:03:09+02:00</th>\n",
       "      <td>1.079</td>\n",
       "      <td>1.209</td>\n",
       "      <td>1.179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-30 00:04:10+02:00</th>\n",
       "      <td>1.079</td>\n",
       "      <td>1.209</td>\n",
       "      <td>1.179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-30 00:10:09+02:00</th>\n",
       "      <td>1.079</td>\n",
       "      <td>1.209</td>\n",
       "      <td>1.179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-30 00:12:10+02:00</th>\n",
       "      <td>1.079</td>\n",
       "      <td>1.209</td>\n",
       "      <td>1.179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">ff55d404-3609-48b2-b16a-ef4a9f2008a0</th>\n",
       "      <th>2020-03-30 23:06:05+02:00</th>\n",
       "      <td>1.059</td>\n",
       "      <td>1.199</td>\n",
       "      <td>1.169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-30 23:07:09+02:00</th>\n",
       "      <td>1.059</td>\n",
       "      <td>1.199</td>\n",
       "      <td>1.169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-30 23:15:10+02:00</th>\n",
       "      <td>1.059</td>\n",
       "      <td>1.199</td>\n",
       "      <td>1.169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-30 23:16:09+02:00</th>\n",
       "      <td>1.059</td>\n",
       "      <td>1.199</td>\n",
       "      <td>1.169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-30 23:43:09+02:00</th>\n",
       "      <td>1.059</td>\n",
       "      <td>1.199</td>\n",
       "      <td>1.169</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>77652 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                diesel     e5   \n",
       "station                              date                                       \n",
       "005056ba-7cb6-1ed2-bceb-7e82e4910d2a 2020-03-30 00:02:10+02:00   1.079  1.209  \\\n",
       "                                     2020-03-30 00:03:09+02:00   1.079  1.209   \n",
       "                                     2020-03-30 00:04:10+02:00   1.079  1.209   \n",
       "                                     2020-03-30 00:10:09+02:00   1.079  1.209   \n",
       "                                     2020-03-30 00:12:10+02:00   1.079  1.209   \n",
       "...                                                                ...    ...   \n",
       "ff55d404-3609-48b2-b16a-ef4a9f2008a0 2020-03-30 23:06:05+02:00   1.059  1.199   \n",
       "                                     2020-03-30 23:07:09+02:00   1.059  1.199   \n",
       "                                     2020-03-30 23:15:10+02:00   1.059  1.199   \n",
       "                                     2020-03-30 23:16:09+02:00   1.059  1.199   \n",
       "                                     2020-03-30 23:43:09+02:00   1.059  1.199   \n",
       "\n",
       "                                                                  e10  \n",
       "station                              date                              \n",
       "005056ba-7cb6-1ed2-bceb-7e82e4910d2a 2020-03-30 00:02:10+02:00  1.179  \n",
       "                                     2020-03-30 00:03:09+02:00  1.179  \n",
       "                                     2020-03-30 00:04:10+02:00  1.179  \n",
       "                                     2020-03-30 00:10:09+02:00  1.179  \n",
       "                                     2020-03-30 00:12:10+02:00  1.179  \n",
       "...                                                               ...  \n",
       "ff55d404-3609-48b2-b16a-ef4a9f2008a0 2020-03-30 23:06:05+02:00  1.169  \n",
       "                                     2020-03-30 23:07:09+02:00  1.169  \n",
       "                                     2020-03-30 23:15:10+02:00  1.169  \n",
       "                                     2020-03-30 23:16:09+02:00  1.169  \n",
       "                                     2020-03-30 23:43:09+02:00  1.169  \n",
       "\n",
       "[77652 rows x 3 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Cannot save file into a non-existent directory: 'D:\\repos\\jurassic-juice-juggler\\data_processed\\stations'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[80], line 34\u001b[0m\n\u001b[0;32m     30\u001b[0m             df\u001b[39m.\u001b[39mto_csv(file_path, mode\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39ma\u001b[39m\u001b[39m'\u001b[39m, header\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, index\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m     33\u001b[0m closing_prices_path \u001b[39m=\u001b[39m META_DIR \u001b[39m/\u001b[39m \u001b[39m'\u001b[39m\u001b[39mclosing_prices.csv\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m---> 34\u001b[0m save_closing_prices(closing_price, closing_prices_path)\n",
      "Cell \u001b[1;32mIn[80], line 14\u001b[0m, in \u001b[0;36msave_closing_prices\u001b[1;34m(df, file_path, date)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[39m# If the file doesn't exist, write the DataFrame to a new CSV file\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m file_path\u001b[39m.\u001b[39mis_file():        \n\u001b[1;32m---> 14\u001b[0m     df\u001b[39m.\u001b[39;49mto_csv(file_path, index\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m     16\u001b[0m \u001b[39m# If it does exist, compare the last line of the CSV File with the last line of the DataFrame df\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     18\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(file_path, \u001b[39m\"\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m file:\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\fuel_env\\lib\\site-packages\\pandas\\core\\generic.py:3772\u001b[0m, in \u001b[0;36mNDFrame.to_csv\u001b[1;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[0;32m   3761\u001b[0m df \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m, ABCDataFrame) \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mto_frame()\n\u001b[0;32m   3763\u001b[0m formatter \u001b[39m=\u001b[39m DataFrameFormatter(\n\u001b[0;32m   3764\u001b[0m     frame\u001b[39m=\u001b[39mdf,\n\u001b[0;32m   3765\u001b[0m     header\u001b[39m=\u001b[39mheader,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3769\u001b[0m     decimal\u001b[39m=\u001b[39mdecimal,\n\u001b[0;32m   3770\u001b[0m )\n\u001b[1;32m-> 3772\u001b[0m \u001b[39mreturn\u001b[39;00m DataFrameRenderer(formatter)\u001b[39m.\u001b[39;49mto_csv(\n\u001b[0;32m   3773\u001b[0m     path_or_buf,\n\u001b[0;32m   3774\u001b[0m     lineterminator\u001b[39m=\u001b[39;49mlineterminator,\n\u001b[0;32m   3775\u001b[0m     sep\u001b[39m=\u001b[39;49msep,\n\u001b[0;32m   3776\u001b[0m     encoding\u001b[39m=\u001b[39;49mencoding,\n\u001b[0;32m   3777\u001b[0m     errors\u001b[39m=\u001b[39;49merrors,\n\u001b[0;32m   3778\u001b[0m     compression\u001b[39m=\u001b[39;49mcompression,\n\u001b[0;32m   3779\u001b[0m     quoting\u001b[39m=\u001b[39;49mquoting,\n\u001b[0;32m   3780\u001b[0m     columns\u001b[39m=\u001b[39;49mcolumns,\n\u001b[0;32m   3781\u001b[0m     index_label\u001b[39m=\u001b[39;49mindex_label,\n\u001b[0;32m   3782\u001b[0m     mode\u001b[39m=\u001b[39;49mmode,\n\u001b[0;32m   3783\u001b[0m     chunksize\u001b[39m=\u001b[39;49mchunksize,\n\u001b[0;32m   3784\u001b[0m     quotechar\u001b[39m=\u001b[39;49mquotechar,\n\u001b[0;32m   3785\u001b[0m     date_format\u001b[39m=\u001b[39;49mdate_format,\n\u001b[0;32m   3786\u001b[0m     doublequote\u001b[39m=\u001b[39;49mdoublequote,\n\u001b[0;32m   3787\u001b[0m     escapechar\u001b[39m=\u001b[39;49mescapechar,\n\u001b[0;32m   3788\u001b[0m     storage_options\u001b[39m=\u001b[39;49mstorage_options,\n\u001b[0;32m   3789\u001b[0m )\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\fuel_env\\lib\\site-packages\\pandas\\io\\formats\\format.py:1186\u001b[0m, in \u001b[0;36mDataFrameRenderer.to_csv\u001b[1;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[0;32m   1165\u001b[0m     created_buffer \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m   1167\u001b[0m csv_formatter \u001b[39m=\u001b[39m CSVFormatter(\n\u001b[0;32m   1168\u001b[0m     path_or_buf\u001b[39m=\u001b[39mpath_or_buf,\n\u001b[0;32m   1169\u001b[0m     lineterminator\u001b[39m=\u001b[39mlineterminator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1184\u001b[0m     formatter\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfmt,\n\u001b[0;32m   1185\u001b[0m )\n\u001b[1;32m-> 1186\u001b[0m csv_formatter\u001b[39m.\u001b[39;49msave()\n\u001b[0;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m created_buffer:\n\u001b[0;32m   1189\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(path_or_buf, StringIO)\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\fuel_env\\lib\\site-packages\\pandas\\io\\formats\\csvs.py:240\u001b[0m, in \u001b[0;36mCSVFormatter.save\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    236\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    237\u001b[0m \u001b[39mCreate the writer & save.\u001b[39;00m\n\u001b[0;32m    238\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    239\u001b[0m \u001b[39m# apply compression and byte/text conversion\u001b[39;00m\n\u001b[1;32m--> 240\u001b[0m \u001b[39mwith\u001b[39;00m get_handle(\n\u001b[0;32m    241\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfilepath_or_buffer,\n\u001b[0;32m    242\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmode,\n\u001b[0;32m    243\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoding,\n\u001b[0;32m    244\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49merrors,\n\u001b[0;32m    245\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompression,\n\u001b[0;32m    246\u001b[0m     storage_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstorage_options,\n\u001b[0;32m    247\u001b[0m ) \u001b[39mas\u001b[39;00m handles:\n\u001b[0;32m    248\u001b[0m     \u001b[39m# Note: self.encoding is irrelevant here\u001b[39;00m\n\u001b[0;32m    249\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwriter \u001b[39m=\u001b[39m csvlib\u001b[39m.\u001b[39mwriter(\n\u001b[0;32m    250\u001b[0m         handles\u001b[39m.\u001b[39mhandle,\n\u001b[0;32m    251\u001b[0m         lineterminator\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlineterminator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    256\u001b[0m         quotechar\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mquotechar,\n\u001b[0;32m    257\u001b[0m     )\n\u001b[0;32m    259\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_save()\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\fuel_env\\lib\\site-packages\\pandas\\io\\common.py:737\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    735\u001b[0m \u001b[39m# Only for write methods\u001b[39;00m\n\u001b[0;32m    736\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m mode \u001b[39mand\u001b[39;00m is_path:\n\u001b[1;32m--> 737\u001b[0m     check_parent_directory(\u001b[39mstr\u001b[39;49m(handle))\n\u001b[0;32m    739\u001b[0m \u001b[39mif\u001b[39;00m compression:\n\u001b[0;32m    740\u001b[0m     \u001b[39mif\u001b[39;00m compression \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mzstd\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    741\u001b[0m         \u001b[39m# compression libraries do not like an explicit text-mode\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\fuel_env\\lib\\site-packages\\pandas\\io\\common.py:600\u001b[0m, in \u001b[0;36mcheck_parent_directory\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m    598\u001b[0m parent \u001b[39m=\u001b[39m Path(path)\u001b[39m.\u001b[39mparent\n\u001b[0;32m    599\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m parent\u001b[39m.\u001b[39mis_dir():\n\u001b[1;32m--> 600\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mOSError\u001b[39;00m(\u001b[39mrf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCannot save file into a non-existent directory: \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mparent\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mOSError\u001b[0m: Cannot save file into a non-existent directory: 'D:\\repos\\jurassic-juice-juggler\\data_processed\\stations'"
     ]
    }
   ],
   "source": [
    "# ADD PRICE CHANGES PER DAY FOR EACH STATION TO THE CLOSING TABLE\n",
    "# CONVERT DATE TO ONLY DAY-DATE\n",
    "# APPEND TO THE EXISTING 'CLOSING_PRICES.CSV'\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import os\n",
    "from collections import deque\n",
    "\n",
    "def save_closing_prices(df, file_path, date='date'):\n",
    "    file_path = Path(file_path)\n",
    "    \n",
    "    # If the file doesn't exist, write the DataFrame to a new CSV file\n",
    "    if not file_path.is_file():        \n",
    "        df.to_csv(file_path, index=True)\n",
    "\n",
    "    # If it does exist, compare the last line of the CSV File with the last line of the DataFrame df\n",
    "    else:\n",
    "        with open(file_path, \"r\") as file:\n",
    "            last_line = deque(file, 1)[0]\n",
    "\n",
    "        # Making sure the lines format is comparable \n",
    "        # CURRENTLY ONLY WORKS WITH DATE ON COLUMN INDEX 1\n",
    "        old_timestamp = pd.to_datetime(last_line.split(',')[1])\n",
    "        new_timestamp = pd.to_datetime(df[date].max())\n",
    "        \n",
    "        # If the new data is not already in the CSV File, append the DataFrame and safe the CSV file.\n",
    "        if new_timestamp <= old_timestamp:\n",
    "            print(\"Some data already exists in the CSV file. Data was not appended.\")\n",
    "        else:\n",
    "            df.to_csv(file_path, mode='a', header=False, index=True)\n",
    "\n",
    "\n",
    "closing_prices_path = META_DIR / 'closing_prices.csv'\n",
    "save_closing_prices(closing_price, closing_prices_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_meta_table(df):\n",
    "    # if csv exists, just open that\n",
    "    # if not create a new pd.DataFrame\n",
    "    # add #observations\n",
    "    # add \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE A METATABLE WITH DAILY SUMMARY:\n",
    "# - ACTIVE STATIONS\n",
    "# - NUMBER OF TIMESTAMPS\n",
    "def add_to_meta_table():\n",
    "    # open closing table file\n",
    "    # append daily meta DataFrame\n",
    "    # save file\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# active_stations = prices_data09.station_uuid.unique()\n",
    "# active_stations_sample = np.random.choice(active_stations, size=100)\n",
    "# pds = prices_df.query('station_uuid in @active_stations_sample')\n",
    "# pds\n",
    "\n",
    "\n",
    "\n",
    "# create a table that carries all stations for each hour of the day\n",
    "\n",
    "# group by the hour of the day, take the average price if a station is occuring more than once during that time\n",
    "\n",
    "# if a station occurs, check the *change columns if its a 2 or a 3, and if yes, check if the price is actually different from the previous hour of if prices have just been re-reported\n",
    "\n",
    "# if prices changed then make a 1 in the price-changed-dummies\n",
    "\n",
    "# if there are multiple occurences of the same station within one hour, check which prices changed and make en entry for the respective dummy\n",
    "\n",
    "# if there are multiple occurences of the same station within one hour, for each of the 3 fuel prices, count how often it changed\n",
    "\n",
    "# take a batch for each hour of the day\n",
    "\n",
    "# check \n",
    "\n",
    "# df = pd.merge(df, pds, how='left', on=['date', 'station_uuid']).set_index(['date', 'station_uuid'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fuel_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
